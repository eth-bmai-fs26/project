{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfdeutIajV89"
      },
      "source": [
        "# üß≠ Project CX: Travel agent tool classifier\n",
        "\n",
        "Welcome to your first ML project coding exercise of the week! You are building an intelligent routing system that solves a real travel industry challenge: automatically directing booking requests to the right service API.\n",
        "\n",
        "A user might type *\"Find me a hotel in Zurich for the weekend\"* or *\"Book a direct flight to Tokyo\"* ‚Äî and your classifier needs to instantly decide which backend tool to call: a flight booker, a hotel booker, or a car rental service. This is the core intelligence behind modern agentic travel apps.\n",
        "\n",
        "**Pipeline:**\n",
        "\n",
        "You'll walk through the complete Machine Learning Engineer pipeline:\n",
        "\n",
        "1. üóÇÔ∏è **Load the data** ‚Äî Get your travel booking dataset ready\n",
        "2. üîç **Inspect the dataset** ‚Äî Understand what you're working with\n",
        "3. üìä **Visualize the embeddings** ‚Äî See your data in 2D space using PCA\n",
        "4. üß± **Define architecture** ‚Äî Build your model, dataset, and metrics\n",
        "5. üèãÔ∏è **Train the classifier** ‚Äî Watch your MLP learn the routing patterns\n",
        "6. üìã **Evaluate performance** ‚Äî Test how well it routes on unseen data\n",
        "7. üöÄ **Live routing demo** ‚Äî Try it yourself with real travel requests!\n",
        "\n",
        "By the end, you will have trained an MLP classifier that intelligently routes travel plans between the 3 booking classes: flights, hotels, and activities. This is the core intelligence behind agentic travel apps where users describe their dream vacation and the system automatically books everything with a single click."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVtRBd5fjV8-"
      },
      "source": [
        "---\n",
        "## 1 üóÇÔ∏è Load the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Omq78FcijV8_"
      },
      "source": [
        "We load pre-computed 384-dim sentence embeddings from CSVs from the `data-generation/` directory. Each row contains an embedding vector (columns `\"0\"`‚Äì`\"383\"`), a string label (`Flight`, `Hotel`, or `CarRental`), and the original sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kJZCwmRjV8_",
        "outputId": "c71535c7-45b6-4526-d38b-d27617f0409f"
      },
      "outputs": [],
      "source": [
        "# Run this cell to download the dataset and clone the repo\n",
        "!git clone -b week1/tool-picker/demo2 https://github.com/eth-bmai-fs26/project.git\n",
        "!git fetch && git checkout week1/tool-picker/demo2\n",
        "%cd project/week1/tool-picker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKYSmtYTjV9A"
      },
      "outputs": [],
      "source": [
        "import os, sys\n",
        "\n",
        "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
        "if PROJECT_ROOT not in sys.path:\n",
        "    sys.path.insert(0, PROJECT_ROOT)\n",
        "\n",
        "DATA_DIR = os.path.join(os.getcwd(), \"data\")\n",
        "\n",
        "# Store file paths for each data split (train / validation / test)\n",
        "train_path = os.path.join(DATA_DIR, \"train.csv\")\n",
        "val_path = os.path.join(DATA_DIR, \"val.csv\")\n",
        "test_path = os.path.join(DATA_DIR, \"test.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3Sac8x8jV9A"
      },
      "source": [
        "---\n",
        "\n",
        "### How did sentences become numbers?\n",
        "\n",
        "Before training a classifier, we need to turn each sentence into something a model can work with: a list of numbers called an **embedding**.\n",
        "\n",
        "Our classifier will work by learning to categorize these sentence embeddings rather than the raw text itself.\n",
        "\n",
        "Think of it like this:\n",
        "\n",
        "> **\"Book a flight to Paris\"** ‚Üí `[0.12, -0.03, 0.47, ‚Ä¶, 0.08]` (384 numbers)\n",
        "\n",
        "A small pretrained language model reads the sentence and compresses its meaning into a fixed-size vector of 384 numbers. Sentences that mean similar things end up with similar vectors. For example, *\"Reserve a plane ticket to Rome\"* would land close to *\"Book a flight to Paris\"* in this 384-dimensional space, while *\"Find me a hotel downtown\"* would be farther away.\n",
        "\n",
        "You don't need to run this step yourself. It was already done ahead of time, and the CSVs you loaded above contain the resulting embedding vectors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VicbsU0ljV9A"
      },
      "source": [
        "---\n",
        "## 2 üîç Load & Inspect"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMdwS95ojV9B"
      },
      "source": [
        "Read the three CSVs into DataFrames and print shapes, class distribution, and a preview of the first few rows to verify the data looks correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 863
        },
        "id": "3Sg2doqWjV9B",
        "outputId": "6fa2ed9c-d7b3-4f08-abfd-3d3dee1d752e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "LABEL_NAMES = {0: \"FLIGHT_BOOKER\", 1: \"HOTEL_BOOKER\", 2: \"CAR_RENTAL_BOOKER\"}\n",
        "LABEL_MAP = {\"Flight\": 0, \"Hotel\": 1, \"CarRental\": 2}\n",
        "\n",
        "train_df = pd.read_csv(train_path, index_col=0)\n",
        "val_df   = pd.read_csv(val_path,   index_col=0)\n",
        "test_df  = pd.read_csv(test_path,  index_col=0)\n",
        "\n",
        "for df in (train_df, val_df, test_df):\n",
        "    df[\"label\"] = df[\"label\"].map(LABEL_MAP)\n",
        "\n",
        "print(\"Overview of the dataset:\")\n",
        "print(f\"Train shape: {train_df.shape}\")\n",
        "print(f\"Val shape:   {val_df.shape}\")\n",
        "print(f\"Test shape:  {test_df.shape}\")\n",
        "print(f\"\\nClass distribution (train):\")\n",
        "print(train_df[\"label\"].value_counts().sort_index().rename(LABEL_NAMES))\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0KTAXmljV9B"
      },
      "source": [
        "#### üîé What to look for here\n",
        "\n",
        "A few things to notice in the output above:\n",
        "\n",
        "- **Shape `(4664, 386)`** ‚Äî 4664 training samples, each with 384 embedding dimensions + the `label` column + the `sentence` column. The validation and test sets are much smaller (583 each), which is a common 80/10/10 split.\n",
        "- **Balanced classes** ‚Äî Each class has roughly 1555 samples in training (~33% each). This is great! With balanced classes we don't need to worry about the model favoring one class over others, and plain **accuracy** is a fair metric. If classes were imbalanced (e.g. 90% flights, 5% hotels, 5% cars), accuracy would be misleading and we'd need metrics like F1-score or balanced accuracy.\n",
        "- **The DataFrame preview** ‚Äî Each row is one training example. The `sentence` column holds the original text, `label` is the integer class (0/1/2), and columns `0` through `383` hold the 384 embedding values. The embedding values are small floating-point numbers roughly in the range [-0.15, 0.15], which is typical for normalized sentence embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnSFZeKljV9B"
      },
      "source": [
        "---\n",
        "## 3 üìä - üéØ: Visualize the embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_MYxZlOjV9C"
      },
      "source": [
        "**Exercise:** Project the 384-dim embeddings down to 2D so we can visualize them.\n",
        "\n",
        "Your task:\n",
        "1. Create a `PCA` object that reduces to **2 components** (use `random_state=42`)\n",
        "2. Call `fit_transform` on `X_train` to get the 2D coordinates\n",
        "\n",
        "The plotting code is already provided ‚Äî once you fill in the two lines, run the cell to see your scatter plot!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "3zha3nInjV9C",
        "outputId": "fbfee397-b7f4-4ec4-ea6a-d8fca9112dbb"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "emb_cols = []\n",
        "for col_name in train_df.columns:\n",
        "    if str(col_name).isdigit():\n",
        "        emb_cols.append(col_name)\n",
        "emb_cols.sort(key=int)\n",
        "X_train = train_df[emb_cols].values\n",
        "y_train = train_df[\"label\"].values\n",
        "\n",
        "# üéØ TODO: Create a PCA object that projects down to 2 components (you only need to fill in the n_components and random_state arguments)\n",
        "# Hint: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA\n",
        "pca = ...\n",
        "\n",
        "# üéØ TODO: Call fit_transform method of pca on X_train to get the 2D projection\n",
        "# Hint: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA.fit_transform\n",
        "X_2d = ...\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "for k in range(3):\n",
        "    mask = y_train == k\n",
        "    plt.scatter(X_2d[mask, 0], X_2d[mask, 1], label=LABEL_NAMES[k], alpha=0.5, s=15)\n",
        "\n",
        "plt.xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]:.1%} var)\")\n",
        "plt.ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]:.1%} var)\")\n",
        "plt.title(\"Training Embeddings ‚Äî PCA Projection\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlqzvxKujV9C"
      },
      "source": [
        "#### üìñ Reading the PCA plot\n",
        "\n",
        "**What is PCA doing here?** Each training sample lives in a 384-dimensional space (one dimension per embedding feature). We obviously can't visualize 384 dimensions, so PCA (Principal Component Analysis) finds the two directions in that space that capture the most variance and projects every point onto them. Think of it as finding the \"best camera angle\" to photograph a 3D object in 2D ‚Äî except here we're going from 384D to 2D.\n",
        "\n",
        "**What to look for:**\n",
        "- **Distinct clusters** ‚Äî If the three classes form well-separated blobs, that's a strong signal that a simple classifier will work well. The embedding model has already done most of the heavy lifting by mapping semantically similar sentences to nearby vectors.\n",
        "- **Overlap between clusters** ‚Äî Where clusters overlap, those are the regions where the classifier will struggle. Misclassified samples at test time will almost always come from these boundary regions.\n",
        "- **Explained variance** ‚Äî The axis labels show how much of the total variance each principal component captures (e.g. \"PC1 (15.2% var)\"). Don't be alarmed if these percentages seem low ‚Äî with 384 dimensions, the information is spread across many axes. Even 10-15% per axis is meaningful.\n",
        "\n",
        "> **Key insight:** PCA is just a visualization tool here, not a preprocessing step. The classifier will train on the full 384-dim embeddings and can exploit structure in all dimensions, not just the two shown in the plot."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNau0mdujV9C"
      },
      "source": [
        "---\n",
        "## 4 üß± Model, Dataset & Metrics Definitions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJC3UezOjV9D"
      },
      "source": [
        "### 4.1 üóÉÔ∏è PyTorch Dataset + DataLoaders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTj9mZ8kjV9D"
      },
      "source": [
        "We import `make_dataloaders` from `dataset.py`, which handles reading the CSVs and packaging each split (train/val/test) into a PyTorch `DataLoader`. Each `DataLoader` serves data in mini-batches during training and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOxzBaH3jV9D",
        "outputId": "6d78535f-ea8d-4467-c075-61fe388d0d4e"
      },
      "outputs": [],
      "source": [
        "from lib.dataset import make_dataloaders\n",
        "\n",
        "print(\"‚úÖ Dataset & DataLoader ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFCMzSFAjV9D"
      },
      "source": [
        "#### üí° Why wrap data in a PyTorch Dataset?\n",
        "\n",
        "You might wonder why we don't just pass NumPy arrays directly to the model. The `Dataset` + `DataLoader` pattern gives us several things for free:\n",
        "\n",
        "1. **Batching** ‚Äî Instead of feeding all 4664 samples at once (which works here but not with larger datasets), the `DataLoader` serves them in mini-batches of 64. This controls memory usage and provides the stochastic gradient updates that help training.\n",
        "2. **Shuffling** ‚Äî The training `DataLoader` shuffles data each epoch, so the model doesn't memorize the order of examples. The validation and test loaders don't shuffle because evaluation order doesn't matter.\n",
        "3. **Automatic tensor conversion** ‚Äî The dataset returns PyTorch tensors ready for GPU/CPU computation, keeping the data pipeline clean.\n",
        "4. **Scalability** ‚Äî This same pattern scales from thousands of samples to millions. For huge datasets you could load from disk lazily instead of holding everything in memory.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fkw7_I8tjV9D"
      },
      "source": [
        "### 4.2 üß† - üéØ Building the Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-ggPZfzjV9D"
      },
      "source": [
        "A minimal two-layer MLP: `Linear(d ‚Üí hidden) ‚Üí ReLU ‚Üí Dropout ‚Üí Linear(hidden ‚Üí 3)`. The output layer produces one score per class ‚Äî the highest score wins."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZSY2RZ3jV9D",
        "outputId": "0690ddc5-5234-4197-9bf1-b832a67679e3"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class ToolRouterMLP(nn.Module):\n",
        "    \"\"\"Simple two-layer MLP: Linear ‚Üí ReLU ‚Üí Dropout ‚Üí Linear.\n",
        "\n",
        "    Outputs raw logits.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim=128, num_classes=3, dropout_p=0.1):\n",
        "        super().__init__()\n",
        "        # üéØ TODO: Inside the nn.Sequential define the network architecture using the provided arguments\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(..., ...),\n",
        "            ...,\n",
        "            nn.Dropout(...),\n",
        "            ...\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # üéØ TODO: return the output of passing x through the network called self.network\n",
        "        return ...\n",
        "\n",
        "\n",
        "print(\"‚úÖ ToolRouterMLP ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCWZUknjjV9E"
      },
      "source": [
        "#### üèóÔ∏è Understanding the architecture\n",
        "\n",
        "Let's unpack what each layer does and *why* it's there:\n",
        "\n",
        "| Layer | What it does | Why we need it |\n",
        "|-------|-------------|----------------|\n",
        "| `Linear(384 ‚Üí 128)` | Multiplies the 384-dim input by a weight matrix to produce 128 features | Learns which combinations of embedding dimensions are useful for classification |\n",
        "| `ReLU()` | Replaces negative values with zero: `max(0, x)` | Introduces **non-linearity** ‚Äî without it, stacking linear layers is mathematically equivalent to a single linear layer, so the network couldn't learn complex decision boundaries |\n",
        "| `Dropout(0.1)` | Randomly zeroes 10% of neurons during training | **Regularization** ‚Äî prevents the network from relying too heavily on any single neuron, reducing overfitting |\n",
        "| `Linear(128 ‚Üí 3)` | Maps 128 hidden features to 3 output scores (one per class) | Produces the final prediction ‚Äî the class with the highest score wins |\n",
        "\n",
        "**Why no softmax at the end?** The last layer outputs one raw score per class. PyTorch's loss function handles converting these scores into probabilities internally, so we don't need to add that step ourselves.\n",
        "\n",
        "**Why only two layers?** Our input embeddings are already highly informative (a pretrained language model produced them). The classifier just needs to learn a relatively simple decision boundary in embedding space. Adding more layers would risk overfitting on our small dataset of 480 samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTlXqRbojV9E"
      },
      "source": [
        "### 4.3 üìê Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYKXf7E_jV9E"
      },
      "source": [
        "Two simple evaluation tools: `accuracy` (how many predictions are correct) and `confusion_matrix` (shows which classes get confused with each other)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "zxACPHkYjV9E"
      },
      "outputs": [],
      "source": [
        "def accuracy(y_true, y_pred) -> float:\n",
        "    \"\"\"Fraction of correct predictions.\"\"\"\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_pred = np.asarray(y_pred)\n",
        "    num_correct = (y_true == y_pred).sum()\n",
        "    return float(num_correct) / len(y_true)\n",
        "\n",
        "\n",
        "from lib.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGY51bSjjV9E"
      },
      "source": [
        "#### üìä Why these two metrics?\n",
        "\n",
        "- **Accuracy** is the go-to metric when classes are balanced (and ours are, at ~33% each). It simply answers: *\"What fraction of predictions are correct?\"* With 3 balanced classes, a random-guessing baseline would score ~33%, so anything above that shows the model has learned something.\n",
        "\n",
        "- **Confusion matrix** tells us *where* the model makes mistakes. Each cell `[i, j]` counts how many samples from true class `i` were predicted as class `j`. The diagonal shows correct predictions; off-diagonal entries are errors. For example, if many hotel bookings get misclassified as car rentals, the confusion matrix will reveal that pattern ‚Äî something overall accuracy alone would hide."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPmlaD5JjV9E"
      },
      "source": [
        "---\n",
        "## 5 üèãÔ∏è Train the classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGs-UJccjV9E"
      },
      "source": [
        "### 5.1 ‚öôÔ∏è Hyperparameters & Reproducibility"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntdM-Gj4jV9E"
      },
      "source": [
        "Set all training hyperparameters in one place and fix random seeds so you get the same results every time you run the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXzwxWgIjV9E",
        "outputId": "615098e5-7bc9-4d7b-9f97-8c1c25547b34"
      },
      "outputs": [],
      "source": [
        "from lib.utils import set_seed\n",
        "import torch\n",
        "\n",
        "SEED       = 42\n",
        "EPOCHS     = 20\n",
        "BATCH_SIZE = 64\n",
        "HIDDEN_DIM = 128\n",
        "LR         = 1e-3\n",
        "DROPOUT_P  = 0.1\n",
        "\n",
        "set_seed(SEED)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"üñ•Ô∏è  Device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrWjaRprjV9F"
      },
      "source": [
        "#### üéõÔ∏è Why these specific values?\n",
        "\n",
        "| Hyperparameter | Value | Rationale |\n",
        "|---|---|---|\n",
        "| `SEED = 42` | Fixed seed | Ensures you get the same results every run ‚Äî critical for debugging and reproducibility |\n",
        "| `EPOCHS = 20` | 20 passes over the training data | Enough for the loss to converge on this dataset without excessive training time |\n",
        "| `BATCH_SIZE = 64` | 64 samples per gradient update | With 4664 training samples, that's ~73 batches per epoch ‚Äî a good balance between stable gradients and frequent updates |\n",
        "| `HIDDEN_DIM = 128` | 128 hidden neurons | Gives the network enough capacity to learn the routing patterns without being so large it overfits |\n",
        "| `LR = 1e-3` | Learning rate of 0.001 | The default for Adam and a solid starting point ‚Äî too high causes instability, too low makes training painfully slow |\n",
        "| `DROPOUT_P = 0.1` | 10% dropout rate | Light regularization as a starting point; heavier dropout (e.g. 0.3‚Äì0.5) is worth exploring if you observe overfitting |\n",
        "\n",
        "> **Tip:** In practice, hyperparameter tuning is often the difference between a mediocre and a great model. Common strategies include grid search, random search, or more sophisticated methods like Bayesian optimization.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsV_K7zWjV9F"
      },
      "source": [
        "### 5.2 üì¶ DataLoaders + Model Init"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQ2UP_4EjV9F"
      },
      "source": [
        "Wrap the three CSVs into PyTorch `DataLoader`s and instantiate the MLP, Adam optimizer, and loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKcd04EejV9F",
        "outputId": "9538b5dd-6949-48cc-d533-628067c93b0e"
      },
      "outputs": [],
      "source": [
        "train_loader, val_loader, test_loader = make_dataloaders(\n",
        "    train_path, val_path, test_path,\n",
        "    batch_size=BATCH_SIZE,\n",
        ")\n",
        "input_dim = train_loader.dataset.dim\n",
        "\n",
        "print(f\"Input dim: {input_dim}\")\n",
        "print(f\"Train: {len(train_loader.dataset)}  \"\n",
        "      f\"Val: {len(val_loader.dataset)}  \"\n",
        "      f\"Test: {len(test_loader.dataset)}\")\n",
        "\n",
        "# Note:     earlier on in part 4.2 you defined the ToolRouterMLP class, now you will create an instance of it\n",
        "#           a class is like a blueprint for our model, and an instance is a specific model created based on that blueprint\n",
        "model = ToolRouterMLP(\n",
        "    input_dim=input_dim,\n",
        "    hidden_dim=HIDDEN_DIM,\n",
        "    num_classes=3,\n",
        "    dropout_p=DROPOUT_P,\n",
        ").to(device)\n",
        "\n",
        "# üéØ TODO: complete the missing parts to define the optimizer\n",
        "# Hint: what is the optimizer trying to optimize? \n",
        "optimizer = torch.optim.Adam(..., lr=LR)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "print(f\"\\n{model}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWlQF9aDjV9F"
      },
      "source": [
        "#### üß© What we just assembled\n",
        "\n",
        "Let's take stock of the pieces we now have ready for training:\n",
        "\n",
        "- **DataLoaders** ‚Äî Three iterators that serve mini-batches of `(embedding, label)` pairs from train/val/test splits\n",
        "- **Model** ‚Äî A 2-layer MLP with 49,539 trainable parameters (`384√ó128 + 128 + 128√ó3 + 3`)\n",
        "- **Optimizer** ‚Äî Adam, which adapts the learning rate per-parameter using running estimates of gradient mean and variance. It's the workhorse optimizer in deep learning for good reason: it works well out of the box for most problems\n",
        "- **Loss function** ‚Äî The standard loss for multi-class classification. It penalizes the model more when it's confidently wrong and rewards confident correct predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVPgO-csjV9F"
      },
      "source": [
        "### 5.3 üîÅ Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xv6XwjlqjV9F"
      },
      "source": [
        "Define `train_one_epoch` and `evaluate` helper functions, then run the full training loop for  `EPOCH` epochs, printing train/val loss and accuracy each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2lPuKmsjV9F",
        "outputId": "673f4640-60ca-405e-b9e5-0fdfce43f065"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, loader, optimizer, loss_fn, device):\n",
        "    \"\"\"Train for one epoch. Returns (avg_loss, accuracy).\"\"\"\n",
        "    model.train() # Set model to training mode\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        # This block is the core of the training loop for one batch!\n",
        "        logits = model(x)           # Forward pass: compute raw class scores (logits) from the model\n",
        "        loss = loss_fn(logits, y)   # Compute cross-entropy loss between predicted logits and true labels\n",
        "        optimizer.zero_grad()       # Reset gradients from the previous batch before backpropagation\n",
        "        loss.backward()             # Backward pass: compute gradients of loss w.r.t. all model parameters\n",
        "        optimizer.step()            # Update model parameters using the computed gradients\n",
        "\n",
        "        # Accumulate weighted loss (multiply by batch size to undo the mean)\n",
        "        batch_size = x.size(0)\n",
        "        total_loss = total_loss + loss.item() * batch_size\n",
        "\n",
        "        # Pick the class with the highest logit as the prediction\n",
        "        preds = logits.argmax(dim=1)\n",
        "        num_correct_in_batch = (preds == y).sum()\n",
        "        correct = correct + num_correct_in_batch.item()\n",
        "        total = total + batch_size\n",
        "\n",
        "    # Return mean loss and fraction of correct predictions over the full epoch\n",
        "    return total_loss / total, correct / total\n",
        "\n",
        "\n",
        "def evaluate(model, loader, loss_fn, device):\n",
        "    \"\"\"Evaluate on a dataset. Returns (avg_loss, accuracy, y_true, y_pred).\"\"\"\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    total_loss = 0.0\n",
        "    all_true = []\n",
        "    all_pred = []\n",
        "\n",
        "    # Disable gradient tracking ‚Äî we only need forward passes during evaluation\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            logits = model(x)\n",
        "            loss = loss_fn(logits, y)\n",
        "\n",
        "            batch_size = x.size(0)\n",
        "            total_loss = total_loss + loss.item() * batch_size\n",
        "\n",
        "            # Predicted class is the index with the highest logit score\n",
        "            preds = logits.argmax(dim=1)\n",
        "            # Collect true labels and predictions for metric computation\n",
        "            all_true.extend(y.cpu().numpy())\n",
        "            all_pred.extend(preds.cpu().numpy())\n",
        "\n",
        "    total = len(all_true)\n",
        "    acc = accuracy(all_true, all_pred)\n",
        "    return total_loss / total, acc, all_true, all_pred\n",
        "\n",
        "\n",
        "# --- run training ---\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    # Train for one full pass over the training set\n",
        "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, loss_fn, device)\n",
        "    # Evaluate on the validation set to monitor generalisation (no weight updates)\n",
        "    val_loss, val_acc, _, _ = evaluate(model, val_loader, loss_fn, device)\n",
        "    print(f\"Epoch {epoch:2d}/{EPOCHS}  \"\n",
        "          f\"train_loss={train_loss:.4f}  train_acc={train_acc:.4f}  \"\n",
        "          f\"val_loss={val_loss:.4f}  val_acc={val_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdvKyh9-jV9G"
      },
      "source": [
        "#### üìà Interpreting the training log\n",
        "\n",
        "Here's what to watch for as you read through the epoch-by-epoch output:\n",
        "\n",
        "1. **Train loss decreasing** ‚Äî The loss should steadily fall from epoch to epoch. If it plateaus early, the learning rate might be too low or the model too small. If it oscillates wildly, the learning rate is too high.\n",
        "\n",
        "2. **Train accuracy climbing** ‚Äî Starting around 72% in epoch 1 and reaching ~98-99% by epoch 20 shows the model is successfully learning the routing patterns.\n",
        "\n",
        "3. **Val loss & accuracy** ‚Äî This is the real report card. The validation set was *never* used for training, so val accuracy reflects how well the model generalizes:\n",
        "   - **Val accuracy ‚âà train accuracy** ‚Üí Good generalization, no significant overfitting\n",
        "   - **Val accuracy << train accuracy** ‚Üí Overfitting ‚Äî the model is memorizing training data rather than learning general patterns. Consider more dropout, fewer epochs, or more training data.\n",
        "   - **Val accuracy > train accuracy** ‚Üí Can happen early on (especially with dropout active during training but not evaluation) and is nothing to worry about.\n",
        "\n",
        "4. **Convergence** ‚Äî Notice how both train and val metrics stabilize toward the later epochs. The model has essentially learned all it can from this data. Training further would yield diminishing returns or even start overfitting.\n",
        "\n",
        "> **Why do we track validation performance during training?** In practice, you'd use val performance to decide *when to stop training* (early stopping) and *which hyperparameters are best*. The test set is only touched once, at the very end, to get an unbiased estimate of real-world performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPJ3dlPWjV9G"
      },
      "source": [
        "---\n",
        "## 6 üìã Evaluate performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkeJHwntjV9G"
      },
      "source": [
        "Run the trained model on the held-out test split and print overall accuracy plus a confusion matrix (rows = true, cols = predicted)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "id": "O2NW0_5djV9G",
        "outputId": "094d765f-a2c6-4b86-9d8b-49b8ec543242"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc, y_true, y_pred = evaluate(model, test_loader, loss_fn, device)\n",
        "\n",
        "print(f\"üìã Test Loss:     {test_loss:.4f}\")\n",
        "print(f\"üìã Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred, num_classes=3)\n",
        "cm_df = pd.DataFrame(\n",
        "    cm,\n",
        "    index=[LABEL_NAMES[i] for i in range(3)],\n",
        "    columns=[LABEL_NAMES[i] for i in range(3)],\n",
        ")\n",
        "print(f\"\\nüìã Confusion Matrix (rows=true, cols=pred):\")\n",
        "cm_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Sk7XYokjV9G"
      },
      "source": [
        "#### üèÜ Interpreting the results\n",
        "\n",
        "**97% test accuracy** ‚Äî Almost every single test sample was routed to the correct tool! The confusion matrix confirms this: the diagonal is filled with the correct counts and all off-diagonal entries are almost zero.\n",
        "\n",
        "**Is this too good to be true?** Not necessarily. Remember:\n",
        "- The sentence embeddings come from a powerful pretrained language model that already captures semantic meaning very well\n",
        "- The three classes (flights, hotels, car rentals) are semantically quite distinct ‚Äî sentences about booking flights sound very different from sentences about renting cars\n",
        "- The dataset is relatively small and clean (synthetically generated)\n",
        "\n",
        "In real-world production, you'd typically see lower accuracy because:\n",
        "- User queries are messy, ambiguous, or multi-intent (\"I need a flight and a hotel\")\n",
        "- The domain boundaries are fuzzier\n",
        "- There may be out-of-distribution inputs that don't belong to any class\n",
        "\n",
        "> **Good practice:** Even with 100% accuracy, always look at the confusion matrix. When accuracy drops (which it will in production), the confusion matrix tells you *which* classes are getting confused, guiding you on where to focus improvement efforts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usk-P4UXjV9G"
      },
      "source": [
        "### 6.1 üî• Confusion Matrix Heatmap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TR2gZnL2jV9G"
      },
      "source": [
        "Render the confusion matrix as a color-coded heatmap for a quick visual read on where the model confuses classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        },
        "id": "W1QFQxHXjV9G",
        "outputId": "664ce6bf-9e38-42bb-8b62-e2ff397ffb0c"
      },
      "outputs": [],
      "source": [
        "from lib.metrics import plot_confusion_matrix\n",
        "plot_confusion_matrix(cm, LABEL_NAMES)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pafct5NdjV9G"
      },
      "source": [
        "#### üó∫Ô∏è How to read the heatmap\n",
        "\n",
        "The heatmap is a visual representation of the same confusion matrix shown as a table above. Here's how to read it:\n",
        "\n",
        "- **Rows** = true (actual) class, **Columns** = predicted class\n",
        "- **Dark diagonal** = correct predictions. The darker/higher the diagonal values, the better.\n",
        "- **Off-diagonal cells** = misclassifications. If cell `(Hotel, Flight)` were dark, it would mean the model often confuses hotel bookings for flight bookings.\n",
        "- **Color intensity** maps to count ‚Äî darker blue means more samples fell into that cell.\n",
        "\n",
        "In our case the heatmap should show a clean diagonal pattern (all predictions correct). In more challenging scenarios, the heatmap immediately reveals patterns like \"car rentals often get confused with hotels\" ‚Äî which would suggest those two classes have similar sentence patterns and might need more training data or better features to distinguish."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "1ntdylEvkwVP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
