{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b427707",
   "metadata": {},
   "source": [
    "# ðŸ‘— From Noise to Fashion â€” Image Generation with Diffusion\n",
    "\n",
    "Welcome! This notebook walks you through building and training an **conditional image generation model** on FashionMNIST: you can tell it which clothing category to generate, and it will create new images of that type from scratch.\n",
    "\n",
    "Today you will build the core component of the \"Graphic Designer Agent\" from the full AI-run magazine application. From thin air, you will be able to generate images of clothes by using a model that learns to reconstruct a corrupted image.\n",
    "\n",
    "This should feel almost like magic! Keep in mind, though, that due to time and resource constraints we will work on much simpler scenarios. The pipeline you build today is the foundation on which modern state-of-the-art image generation models are built. Those models â€” such as Stable Diffusion â€” use the very same approach, but are trained on billions of images across ~256 high-end GPUs for several weeks.\n",
    "\n",
    "**Pipeline:**\n",
    "\n",
    "1. ðŸ‘— **Load the data** â€” Set up FashionMNIST with 10 clothing classes\n",
    "2. ðŸ’¨ **Forward diffusion** â€” Mathematically destroy images by adding Gaussian noise\n",
    "3. ðŸ”¬ **Visualize the noise** â€” Watch a clean image dissolve into static, step by step\n",
    "4. ðŸ§  **Train a score model** â€” Teach a UNet to predict and reverse the noise\n",
    "5. ðŸŽ¨ **Generate new fashion** â€” Sample class-conditional images from pure noise\n",
    "6. ðŸŽžï¸ **Watch it denoise** â€” Visualize the full reverse diffusion journey\n",
    "\n",
    "By the end, you'll be able to prompt the model for any of the 10 clothing categories â€” giving you a hands-on look at the core technology behind modern image generation systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2136cdb2",
   "metadata": {},
   "source": [
    "---\n",
    "## 0 ðŸ“¦ Imports\n",
    "\n",
    "We import PyTorch for building and training the model, torchvision for the FashionMNIST dataset, and matplotlib for visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f1ca8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import utils\n",
    "\n",
    "# --- Set up device. If you don't have a GPU, consider using Google Colab ---\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available(): #Use this if you have a CUDA-compatible GPU (e.g. Nvidida)\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available(): #Use this if you have a Mac with M1/M2/M3/M4 chip\n",
    "    device = \"mps\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4fb5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell to clone the repo for data and demo html on colab\n",
    "!git clone -b week2-fashion-magazine-notebook https://github.com/eth-bmai-fs26/project.git\n",
    "!git fetch && git checkout week2/fashion-magazine/notebook\n",
    "%cd project/week2/notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0650e0",
   "metadata": {},
   "source": [
    "---\n",
    "## 1 ðŸ‘— Load the Data\n",
    "\n",
    "Before we can generate fashion images, we need to show the model what fashion looks like. **FashionMNIST** contains 60,000 grayscale training images across 10 clothing categories, each 28Ã—28 pixels.\n",
    "\n",
    "| Label | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 |\n",
    "|-------|---|---|---|---|---|---|---|---|---|---|\n",
    "| **Class** | T-shirt/top | Trouser | Pullover | Dress | Coat | Sandal | Shirt | Sneaker | Bag | Ankle boot |\n",
    "\n",
    "We normalize each image to the range **[-1, 1]** (from raw [0, 255] pixel values). This matters for diffusion models: the noise process is defined over a zero-centered Gaussian, so keeping pixel values zero-centered keeps everything consistent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79508d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define transformations\n",
    "# Transform to tensor and normalize dataset to [-1, 1]\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)) # (mean, std) for single channel\n",
    "])\n",
    "\n",
    "# FashionMNIST class names (label index -> clothing category)\n",
    "CLASS_NAMES = [\n",
    "    'T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "    'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'\n",
    "]\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "\n",
    "# Mapping from class names to indices (e.g., 'T-shirt/top' -> 0)\n",
    "CLASS_TO_IDX = {name: idx for idx, name in enumerate(CLASS_NAMES)}\n",
    "\n",
    "# --- Load FashionMNIST and create DataLoader (all 10 classes)\n",
    "train_dataset = torchvision.datasets.FashionMNIST(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "batch_size = 1024\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Let's look at one image to confirm\n",
    "image, label = train_dataset[0]\n",
    "print(\"Image shape:\", image.shape)\n",
    "print(f\"Label: {label} ({CLASS_NAMES[label]})\")\n",
    "print(f\"Pixel value range: {image.min()} to {image.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a9a875",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.show_class_examples(train_dataset, CLASS_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12559b64",
   "metadata": {},
   "source": [
    "---\n",
    "## 2 ðŸ’¨ Forward Diffusion\n",
    "\n",
    "The key idea behind diffusion models is elegantly destructive: take a clean image and **gradually add Gaussian noise** over many timesteps until nothing recognizable remains. Then train a neural network to reverse the process.\n",
    "\n",
    "The closed-form equation for the forward process lets us jump to *any* noise level in a single shot â€” no need to apply noise iteratively:\n",
    "\n",
    "$$X_{i} = \\sqrt{\\bar{\\alpha}_{i}} \\, X_{0} + \\sqrt{1-\\bar{\\alpha}_{i}} \\, \\varepsilon, \\;\\;\\; \\varepsilon \\sim \\mathcal{N}(0,I).$$\n",
    "\n",
    "Here $\\bar{\\alpha}_i$ is the **cumulative product of the noise schedule** â€” it controls how much of the original image survives at timestep $i$:\n",
    "- When $\\bar{\\alpha}_i \\approx 1$ (early timesteps) â€” the image is nearly unchanged\n",
    "- When $\\bar{\\alpha}_i \\approx 0$ (late timesteps) â€” the image is almost pure Gaussian noise\n",
    "\n",
    "> **Why is the closed form useful?** Training samples a random timestep $t$ for every image in the batch. Without this formula, computing $x_t$ would require applying the noise step $t$ times in a loop. With it, we get any noise level instantly â€” making training fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8659b18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for the diffusion process\n",
    "TIMESTEPS = 800\n",
    "beta_start = 0.0001\n",
    "beta_end = 0.02\n",
    "\n",
    "# Define the beta schedule (variance)\n",
    "betas = torch.linspace(beta_start, beta_end, TIMESTEPS, device=device)\n",
    "\n",
    "# Pre-calculate alphas and other values for the formula\n",
    "alphas = 1. - betas\n",
    "alphas_bar = torch.cumprod(alphas, dim=0) # Computes comulative product of alphas\n",
    "\n",
    "sqrt_alphas_bar = torch.sqrt(alphas_bar)\n",
    "sqrt_one_minus_alphas_bar = torch.sqrt(1. - alphas_bar)\n",
    "\n",
    "# The Forward Noising Function\n",
    "def add_noise(x_0, t):\n",
    "    \"\"\"\n",
    "    Applies noise to an image x_0 at a specific timestep t.\n",
    "    \"\"\"\n",
    "    # Sample random noise from a standard Gaussian\n",
    "    noise = torch.randn_like(x_0)\n",
    "\n",
    "    # Get the pre-calculated values for the given timesteps\n",
    "    sqrt_alphas_bar_t = get_values_at_timestep(sqrt_alphas_bar, t)\n",
    "    sqrt_one_minus_alphas_bar_t = get_values_at_timestep(sqrt_one_minus_alphas_bar, t)\n",
    "\n",
    "    # Apply the formula to get the noisy image\n",
    "    x_t = sqrt_alphas_bar_t * x_0 + sqrt_one_minus_alphas_bar_t * noise\n",
    "    return x_t, noise\n",
    "\n",
    "# Helper function to get the correct values for a batch of timesteps\n",
    "def get_values_at_timestep(values, t):\n",
    "    \"\"\"\n",
    "    Gathers the values from the `values` tensor at the indices specified by `t`.\n",
    "    Reshapes the output to be compatible for broadcasting with image tensors.\n",
    "    \"\"\"\n",
    "    batch_size = t.shape[0]\n",
    "    # Gather values and reshape to (batch_size, 1, 1, 1) for broadcasting\n",
    "    return values.gather(-1, t).reshape(batch_size, 1, 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2eadf1",
   "metadata": {},
   "source": [
    "---\n",
    "## 3 ðŸ”¬ Visualize the Noising Process\n",
    "\n",
    "Let's see the forward process in action. We'll take a single image and plot it at 10 evenly-spaced timesteps from $t=0$ (clean) to $t=T-1$ (pure noise).\n",
    "\n",
    "**What to look for:**\n",
    "- **Early timesteps** â€” The original image is clearly recognizable, just slightly grainier\n",
    "- **Middle timesteps** â€” Fine details blur; shapes start to melt into the background\n",
    "- **Late timesteps** â€” The image is unrecognizable â€” pure Gaussian noise\n",
    "\n",
    "This progression is exactly what the model has to *undo* during generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4efbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of the forward pass\n",
    "image, _ = train_dataset[0] # Get first image\n",
    "image = image.to(device).unsqueeze(0) # Add batch dimension and move to device\n",
    "\n",
    "num_steps = 10  # Number of noise levels to visualize\n",
    "vis_timesteps = torch.linspace(0, TIMESTEPS - 1, num_steps).long().tolist()\n",
    "noisy_images = []\n",
    "titles = []\n",
    "\n",
    "# Generate noisy images at specified timesteps\n",
    "for timestep in vis_timesteps:\n",
    "    noisy_img, _ = add_noise(image, torch.tensor([timestep], device=device))\n",
    "    noisy_images.append(noisy_img)\n",
    "    titles.append(f\"t = {timestep}\")\n",
    "\n",
    "print(\"Visualizing the forward noising process:\")\n",
    "utils.show_images(torch.cat(noisy_images, dim=0), titles=titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44754278",
   "metadata": {},
   "source": [
    "---\n",
    "## 4 ðŸ§  Reverse Diffusion â€” Training the Denoiser\n",
    "\n",
    "Now for the clever part. We've seen how to *destroy* an image. The question is: can a neural network learn to *reverse* this process?\n",
    "\n",
    "The answer is yes â€” by training a **UNet** to predict the noise $\\varepsilon$ that was added at each timestep. If the model can accurately estimate the noise, it can subtract it to recover a slightly cleaner image. Repeat 800 times starting from pure static, and a coherent image emerges.\n",
    "\n",
    "The model is **class-conditional**: we pass the clothing category label alongside the noisy image, so the model learns to reverse noise in a class-aware way. This is how you steer generation toward a specific category at sampling time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18eb980e",
   "metadata": {},
   "source": [
    "### 4.1 ðŸ‹ï¸ Train the UNet\n",
    "\n",
    "We import `SimpleUNet` â€” a lightweight UNet with skip connections that takes a noisy image $x_t$, the timestep $t$, and the class label as inputs, and outputs a **predicted noise tensor of the same shape**.\n",
    "\n",
    "The training objective is beautifully simple: minimize the MSE between the predicted noise and the actual noise that was added. This is the core of the DDPM training algorithm.\n",
    "\n",
    "| Input | What it is |\n",
    "|-------|-----------|\n",
    "| `x_t` | The noisy image at timestep $t$ |\n",
    "| `t` | The timestep â€” tells the model how noisy the input is |\n",
    "| `labels` | The class label â€” steers denoising toward a specific category |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb28c186",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unet import SimpleUNet\n",
    "\n",
    "# --- Initialize the model which we use to predict the score function.\n",
    "score_model = SimpleUNet(base_ch=64, emb_dim=64, num_classes=NUM_CLASSES).to(device) #Set base_ch to 16 if this takes too long on your laptop\n",
    "score_model.train()\n",
    "optimizer = optim.AdamW(score_model.parameters(), lr=1e-4)\n",
    "\n",
    "# --- Training loop, learning to reverse the noise. ---\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for x_0, labels in train_loader: # CHANGE 2: unpack labels alongside images\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        x_0 = x_0.to(device)\n",
    "        labels = labels.to(device)  # CHANGE 2: move labels to device\n",
    "        batch_dimension = x_0.size(0)\n",
    "        t = torch.randint(0, TIMESTEPS, (batch_dimension,), device=device).long() # choose random timestep for every sample from the input\n",
    "\n",
    "        # Add noise using the forward process\n",
    "        x_t, noise = add_noise(x_0, t)\n",
    "\n",
    "        # Predict the added noise, conditioned on the class label\n",
    "        predicted_noise = score_model(x_t, t, labels)  # CHANGE 2: pass labels to model\n",
    "\n",
    "        # Learn to reverse the noise\n",
    "        loss = F.mse_loss(predicted_noise, noise)\n",
    "\n",
    "        # Take an optimization step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch%1 == 0:\n",
    "        print(f\"Finished epoch {epoch}. Current loss: loss={loss.item():.4f}\")\n",
    "\n",
    "print(\"Training is finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510d01de",
   "metadata": {},
   "source": [
    "### 4.2 ðŸ” The Sampling Equations\n",
    "\n",
    "Training taught the model to estimate noise. Now we use that estimate to iteratively denoise, stepping backwards from $t=T$ to $t=0$.\n",
    "\n",
    "At each step, we compute the **posterior mean** â€” our best guess for a slightly-less-noisy image:\n",
    "\n",
    "$$\\mu(x_i) = \\frac{1}{\\sqrt{\\alpha_i}} \\left( x_i - \\frac{1-\\alpha_i}{\\sqrt{1 - \\bar{\\alpha}_{i}}} \\bar{\\varepsilon}_{i} \\right)$$\n",
    "\n",
    "We also inject a small amount of random noise scaled by the **posterior variance**, to maintain sample diversity:\n",
    "\n",
    "$$\\sigma^{2}(i) = \\frac{(1 - \\alpha_{i}) (1 - \\bar{\\alpha}_{i-1})}{1 - \\bar{\\alpha}_{i}}$$\n",
    "\n",
    "Putting it together, the full reverse step is:\n",
    "\n",
    "$$x_{i-1} = \\frac{1}{\\sqrt{\\alpha_i}} \\left( x_i - \\frac{1-\\alpha_i}{\\sqrt{1 - \\bar{\\alpha}_{i}}} \\bar{\\varepsilon}_{i} \\right) + \\sigma(i) z$$\n",
    "\n",
    "> **Why add noise during sampling?** Without the $\\sigma(i) z$ term, every run from the same starting noise would produce the identical image. The injected stochasticity is what gives diffusion models their rich, diverse outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ibqtxemabms",
   "metadata": {},
   "source": [
    "---\n",
    "## 5 ðŸŽ¨ Generate New Fashion\n",
    "\n",
    "Training is done â€” now comes the fun part. We sample new images by starting from **pure Gaussian noise** and running the reverse diffusion process 800 steps, guided by the trained UNet.\n",
    "\n",
    "Because the model is class-conditional, we can ask it to generate any of the 10 clothing categories. Change `class_label` to any number from 0 to 9 and watch the model invent a new garment from static."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efdb26ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precompute helpful buffers for sampling\n",
    "alphas_bar_previous = torch.cat([torch.tensor([1.0], device=device), alphas_bar[:-1]], dim=0)\n",
    "sqrt_alphas_inverse = torch.sqrt(1.0 / alphas)\n",
    "inverse_sqrt_one_minus_alphas_bar = torch.sqrt(1.0 - alphas_bar)\n",
    "\n",
    "# Posterior mean coefficient\n",
    "coef_eps = (1 - alphas) / inverse_sqrt_one_minus_alphas_bar\n",
    "\n",
    "# Posterior variance\n",
    "posterior_variance = (1 - alphas) * (1 - alphas_bar_previous) / (1 - alphas_bar)\n",
    "posterior_log_variance_clipped = torch.log(torch.clamp(posterior_variance, min=1e-20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a7621b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample(score_model, num_samples=16, class_label=0):\n",
    "    score_model.eval()\n",
    "    # Start from random noise\n",
    "    x_t = torch.randn(num_samples, 1, 28, 28, device=device)\n",
    "\n",
    "    # Class label tensor â€” same label for all samples in this batch\n",
    "    labels = torch.full((num_samples,), class_label, device=device, dtype=torch.long)\n",
    "\n",
    "    # Now we reverse the noising\n",
    "    for timestep in reversed(range(TIMESTEPS)):\n",
    "        t = torch.full((num_samples,), timestep, device=device, dtype=torch.long)\n",
    "\n",
    "        # Denoising, conditioned on the class label\n",
    "        mean = (x_t - coef_eps[timestep] * score_model(x_t, t, labels)) * sqrt_alphas_inverse[timestep]\n",
    "\n",
    "        if timestep > 0:\n",
    "            #Add noise for sample diversity\n",
    "            noise = torch.randn_like(x_t)\n",
    "            var = posterior_variance[timestep]\n",
    "            x_t = mean + torch.sqrt(var) * noise\n",
    "        else:\n",
    "            x_t = mean\n",
    "    return x_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce89b9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 16\n",
    "class_label = 0 # tshirt\n",
    "\n",
    "# Generate samples conditioned on the chosen class\n",
    "samples = sample(score_model, num_samples=num_samples, class_label=class_label)\n",
    "\n",
    "# unnormalize back to [0,1] for display\n",
    "samples = (samples.clamp(-1, 1) + 1) / 2.0\n",
    "\n",
    "print(f\"16 samples of class {class_label}: '{CLASS_NAMES[class_label]}'.\")\n",
    "utils.show_images(samples[:num_samples].cpu(), titles=[CLASS_NAMES[class_label] for _ in range(num_samples)], grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k7fpcq2j3ab",
   "metadata": {},
   "source": [
    "---\n",
    "## 6 ðŸŽžï¸ Watch It Denoise\n",
    "\n",
    "A single image, visualized at 10 checkpoints across the full denoising journey â€” from pure noise all the way to the finished garment.\n",
    "\n",
    "**What to look for:**\n",
    "- **High $t$** â€” Indistinguishable static; the model is just getting started\n",
    "- **Mid $t$** â€” Rough shapes and outlines begin to emerge\n",
    "- **Low $t$** â€” Fine texture and detail snap into place in the final steps\n",
    "\n",
    "This is the reverse of Section 3 â€” the same 10-step window, but running backwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd29781",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def denoising_process_single_sample(\n",
    "    score_model,\n",
    "    class_label: int = 0,                         # which class to generate\n",
    "    record_timesteps: list[int] | None = None,    # which t to keep (e.g., [TIMESTEPS-1,...,0]); None = keep all\n",
    "):\n",
    "    score_model.eval()\n",
    "\n",
    "    x_t = torch.randn(1, 1, 28, 28, device=device)\n",
    "    start_t = TIMESTEPS - 1\n",
    "\n",
    "    # Class label tensor (batch size 1)\n",
    "    labels = torch.tensor([class_label], device=device, dtype=torch.long)\n",
    "\n",
    "    record_set = set(record_timesteps) if record_timesteps is not None else None\n",
    "    trace = []\n",
    "\n",
    "    # Optionally record the starting x_T\n",
    "    if record_set is None or start_t in record_set:\n",
    "        trace.append((start_t, x_t.detach().clone()))\n",
    "\n",
    "    for timestep in reversed(range(TIMESTEPS)):\n",
    "        t = torch.full((1,), timestep, device=device, dtype=torch.long)\n",
    "\n",
    "        # mean term: (x - coef_eps_i * eps_theta) / sqrt(alpha_i), conditioned on class\n",
    "        mean = (x_t - coef_eps[timestep] * score_model(x_t, t, labels)) * sqrt_alphas_inverse[timestep]\n",
    "\n",
    "        if timestep > 0:\n",
    "            noise = torch.randn_like(x_t)\n",
    "            var = posterior_variance[timestep]\n",
    "            x_t = mean + torch.sqrt(var) * noise\n",
    "        else:\n",
    "            x_t = mean  # final step has no noise\n",
    "\n",
    "        if record_set is None or timestep in record_set:\n",
    "            trace.append((timestep, x_t.detach().clone()))\n",
    "\n",
    "    # trace is in descending t order already (T-1 -> 0)\n",
    "    return x_t, trace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cdfbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- Example usage: record specific timesteps to mirror your forward viz ----\n",
    "vis_timesteps = torch.linspace(TIMESTEPS - 1, 0, 10).long().tolist()\n",
    "class_label = 2\n",
    "\n",
    "# Run reverse sampling for one sample and keep only those steps\n",
    "x0, trace = denoising_process_single_sample(score_model, class_label=class_label, record_timesteps=vis_timesteps)\n",
    "\n",
    "# Prepare images (ordered by vis_timesteps for consistent titles)\n",
    "# trace is a list of (t, x_t); convert to dict for quick indexing\n",
    "trace_dict = {t: x_t for (t, x_t) in trace}\n",
    "imgs = []\n",
    "titles = []\n",
    "for t_val in vis_timesteps:\n",
    "    x_t = trace_dict[t_val]\n",
    "    # unnormalize from [-1,1] to [0,1] for display\n",
    "    x_disp = (x_t.clamp(-1, 1) + 1) / 2.0\n",
    "    imgs.append(x_disp.squeeze(0))  # remove batch dim -> (1,28,28)\n",
    "    titles.append(f\"t = {t_val}\")\n",
    "\n",
    "# Visualize the reverse denoising process\n",
    "print(f\"Visualizing the reverse denoising process for class {class_label}: '{CLASS_NAMES[class_label]}':\")\n",
    "utils.show_images(torch.stack(imgs).cpu(), titles=titles)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
